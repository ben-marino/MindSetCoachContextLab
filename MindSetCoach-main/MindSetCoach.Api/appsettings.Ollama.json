{
  "_readme": "Local AI configuration using Ollama - copy to appsettings.Development.json to use",
  
  "AI": {
    "Provider": "Ollama",
    "ModelId": "llama3.1:8b",
    "OllamaEndpoint": "http://localhost:11434"
  },
  
  "_availableModels": {
    "_comment": "Run 'ollama pull <model>' to download these",
    "recommended": [
      "llama3.1:8b        - Best balance of speed/quality (5GB, fits 16GB VRAM)",
      "mistral:7b         - Fast inference, solid quality (4GB)",
      "phi3:medium        - Microsoft 14B, 128k context (8GB)",
      "qwen2.5:7b         - Strong reasoning (4GB)",
      "gemma2:9b          - Google's efficient model (5GB)"
    ],
    "experimental": [
      "mixtral:8x7b-q3    - MoE, pushes VRAM limits (13GB)",
      "llama3.1:70b-q2    - Largest that fits, quality tradeoff (14GB)"
    ]
  }
}
